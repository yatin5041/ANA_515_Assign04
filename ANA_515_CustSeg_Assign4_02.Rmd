---
title: "ANA_515_Assign4_Customer_Segmentation"
author: "Yatin Pawar"
date: "12/10/2021"
output: html_document
---
## What is Customer Segmentation?
Customer Segmentation is the process of division of customer base into several groups of individuals that share a similarity in different ways that are relevant to marketing such as gender, age, interests, and miscellaneous spending habits.

Companies can segment markets in several ways:
Geographically by region or area,
Demographically by age, gender, family size, income, or life cycle,
Psychographically by social class, lifestyle, or personality, and 
Behaviorally by benefit, use, or response

The dataset for this analysis is available [here](https://drive.google.com/file/d/19BOhwz52NUY3dg8XErVYglctpr5sjTy4/view).

```{r setup, include=FALSE}

library(ggplot2)
library(plotrix)
library(purrr)
library(cluster)
library(gridExtra)
library(grid)
library(NbClust)
library(factoextra)
library(knitr)

#We will now retrieve our data from movies.csv into movie_data dataframe and ratings.csv into rating_data.

customer_data=read.csv("Mall_Customers.csv")
str(customer_data)

names(customer_data)

```

The dataset has `r nrow(customer_data)` rows and `r ncol(customer_data)` columns.

# Statistical summary of the Customer data as below:
```{r, echo=FALSE}
sum_customer_data <- summary(customer_data)
print(sum_customer_data)

```

# Data Preparation
This dataset do not have any issues. In general, data preparation involves dealing with missing values, identifying and treating outliers, normalizing and transforming data, and converting data types.

In this dataset, the data must have been corrected for variables:
CustomerID - inconsistent format, alphanumeric inputs, missing values
Gender - inconsistent response like M,F instead of Male, Female, correcting numeric inputs, alphanumeric inputs, categorization of any inputs other than Male and Female, and missing values
Age - non numeric, zero, negative values or in decimal points. Since its a customer data should be at least above 14, missing values 
Annual income - non numeric, zero or negative values, missing values    
Spending score - non numeric, zero or negative values, values less than 1 and greater than 100 as the range is fixed, missing values

# Data Visualization for understanding

We will study all the variables first before proceeding.

First, we will create a barplot and a piechart to show the "gender" distribution across our customer_data dataset.

```{r echo=FALSE}
a=table(customer_data$Gender)
barplot(a,main="Using BarPlot to display Gender Comparision",
       ylab="Count",
       xlab="Gender",
       col=rainbow(2),
       legend=rownames(a))
```

From the above barplot, we observe that the number of female customers is higher than the male customers. Now, let us visualize a pie chart to observe the ratio of male and female distribution.

```{r echo=FALSE}
pct=round(a/sum(a)*100)
lbs=paste(c("Female","Male")," ",pct,"%",sep=" ")
library(plotrix)
pie3D(a,labels=lbs,
   main="Pie Chart Depicting Ratio of Female and Male")
```
From the pie chart, we see that the percentage of females is 56%, whereas the percentage of male in the customer dataset is 44%.

Now let's take a look at variable "Age":
Let us plot a histogram and boxplot to view the distribution to plot the frequency of customer ages.

```{r echo=FALSE}
hist(customer_data$Age,
    col="blue",
    main="Histogram to Show Count of Age Class",
    xlab="Age Class",
    ylab="Frequency",
    labels=TRUE)
```

```{r echo=FALSE}
boxplot(customer_data$Age,
       main="Boxplot for variable Age")
```
From the above two visualizations, we conclude that the maximum customer ages are between 30 and 35. The minimum age of customers is 18, whereas, the maximum age is 70.

Will continue with analysis and will create visualizations for "annual income" of the customers. We will see distribution of "annual income", plot a histogram and then we will proceed to examine this data using a density plot.

```{r echo=FALSE}

summary(customer_data$Annual.Income..k..)

hist(customer_data$Annual.Income..k..,
  col="#660033",
  main="Histogram for Annual Income",
  xlab="Annual Income Class",
  ylab="Frequency",
  labels=TRUE)

```

```{r echo=FALSE}

plot(density(customer_data$Annual.Income..k..),
    col="yellow",
    main="Density Plot for Annual Income",
    xlab="Annual Income Class",
    ylab="Density")
polygon(density(customer_data$Annual.Income..k..),
        col="#ccff66")

```

From the above descriptive analysis, we conclude that the minimum annual income of the customers is 15 and the maximum income is 137. People earning an average income of 70 have the highest frequency count in our histogram distribution. The average salary of all the customers is 60.56. In the Kernel Density Plot that we displayed above, we observe that the annual income has a normal distribution.

Now we will analyze Spending scores of the customers

```{r echo=FALSE}

summary(customer_data$Spending.Score..1.100.)

boxplot(customer_data$Spending.Score..1.100.,
   horizontal=TRUE,
   col="#990000",
   main="BoxPlot for Descriptive Analysis of Spending Score")

```

```{r echo=FALSE}

hist(customer_data$Spending.Score..1.100.,
    main="HistoGram for Spending Score",
    xlab="Spending Score Class",
    ylab="Frequency",
    col="#6600cc",
    labels=TRUE)

```

From the Descriptive Analysis of Spending Score we see that Min is 1, Max is 99 and avg. is 50.20. From the histogram, we conclude that customers between class 40 and 50 have the highest spending score among all the classes.

## K-means Algorithm

K-means clustering is one of the simplest and popular unsupervised machine learning algorithms that is used in segmentation analysis.

The K-means algorithm identifies k number of centroids, and then allocates every data point to the nearest cluster, while keeping the centroids as small as possible. The ‘means’ in the K-means refers to averaging of the data; that is, finding the centroid.

The first step is to indicate the number of clusters (k) that we wish to produce in the final output. The algorithm starts by selecting k objects from dataset randomly that will serve as the initial centers for our clusters. These selected objects are the cluster means, also known as centroids. Then, the remaining objects have an assignment of the closest centroid. This centroid is defined by the Euclidean Distance present between the object and the cluster mean. We refer to this step as “cluster assignment”. When the assignment is complete, the algorithm proceeds to calculate new mean value of each cluster present in the data. After the recalculation of the centers, the observations are checked if they are closer to a different cluster. Using the updated cluster mean, the objects undergo reassignment. This goes on repeatedly through several iterations until the cluster assignments stop altering. The clusters that are present in the current iteration are the same as the ones obtained in the previous iteration.

## Determining Optimum Cluster
While working with clusters, we need to specify the number of clusters to use. We would like to utilize the optimal number of clusters, which can be determined by below methods –

*Elbow method
*Silhouette method

##Elbow Method:

The main goal behind cluster partitioning methods like k-means is to define the clusters such that the intra-cluster variation stays minimum.

First, we calculate the clustering algorithm for several values of k. This can be done by creating a variation within k from 1 to 10 clusters. We then calculate the total intra-cluster sum of square (iss). Then, we proceed to plot iss based on the number of k clusters. This plot denotes the appropriate number of clusters required in our model. In the plot, the location of a bend or a knee is the indication of the optimum number of clusters. 


```{r echo=FALSE}

library(purrr)
set.seed(123)
# function to calculate total intra-cluster sum of square 
iss <- function(k) {
  kmeans(customer_data[,3:5],k,iter.max=100,nstart=100,algorithm="Lloyd" )$tot.withinss
}

k.values <- 1:10

iss_values <- map_dbl(k.values, iss)

plot(k.values, iss_values,
    type="b", pch = 19, frame = FALSE, 
    xlab="Number of clusters K",
    ylab="Total intra-clusters sum of squares")

```

The above graph shows that 4 is the appropriate number of clusters as it seems to be appearing at the bend in the elbow plot.


##Silhouette Method:

With the help of the average silhouette method, we can measure the quality of our clustering operation. With this, we can determine how well within the cluster is the data object. If we obtain a high average silhouette width, it means that we have good clustering. The average silhouette method calculates the mean of silhouette observations for different k values. With the optimal number of k clusters, one can maximize the average silhouette over significant values for k clusters.

Using the silhouette function in the cluster package, we can compute the average silhouette width using the kmean function. Here, the optimal cluster will possess highest average.

```{r echo=FALSE}

library(cluster) 
library(gridExtra)
library(grid)


k2<-kmeans(customer_data[,3:5],2,iter.max=100,nstart=50,algorithm="Lloyd")
s2<-plot(silhouette(k2$cluster,dist(customer_data[,3:5],"euclidean")))

```


```{r echo=FALSE}

k3<-kmeans(customer_data[,3:5],3,iter.max=100,nstart=50,algorithm="Lloyd")
s3<-plot(silhouette(k3$cluster,dist(customer_data[,3:5],"euclidean")))

```


```{r echo=FALSE}

k4<-kmeans(customer_data[,3:5],4,iter.max=100,nstart=50,algorithm="Lloyd")
s4<-plot(silhouette(k4$cluster,dist(customer_data[,3:5],"euclidean")))

```


```{r echo=FALSE}

k5<-kmeans(customer_data[,3:5],5,iter.max=100,nstart=50,algorithm="Lloyd")
s5<-plot(silhouette(k5$cluster,dist(customer_data[,3:5],"euclidean")))

```


```{r echo=FALSE}

k6<-kmeans(customer_data[,3:5],6,iter.max=100,nstart=50,algorithm="Lloyd")
s6<-plot(silhouette(k6$cluster,dist(customer_data[,3:5],"euclidean")))

```


```{r echo=FALSE}

k7<-kmeans(customer_data[,3:5],7,iter.max=100,nstart=50,algorithm="Lloyd")
s7<-plot(silhouette(k7$cluster,dist(customer_data[,3:5],"euclidean")))

```


```{r echo=FALSE}

k8<-kmeans(customer_data[,3:5],8,iter.max=100,nstart=50,algorithm="Lloyd")
s8<-plot(silhouette(k8$cluster,dist(customer_data[,3:5],"euclidean")))

```


```{r echo=FALSE}

k9<-kmeans(customer_data[,3:5],9,iter.max=100,nstart=50,algorithm="Lloyd")
s9<-plot(silhouette(k9$cluster,dist(customer_data[,3:5],"euclidean")))

```


```{r echo=FALSE}

k10<-kmeans(customer_data[,3:5],10,iter.max=100,nstart=50,algorithm="Lloyd")
s10<-plot(silhouette(k10$cluster,dist(customer_data[,3:5],"euclidean")))

```

Now, we make use of the fviz_nbclust() function to determine and visualize the optimal number of clusters as follows –


```{r echo=FALSE}
library(NbClust)
library(factoextra)

fviz_nbclust(customer_data[,3:5], kmeans, method = "silhouette")

```

Now, let us take k = 6 as our optimal cluster –

```{r echo=FALSE}

k6<-kmeans(customer_data[,3:5],6,iter.max=100,nstart=50,algorithm="Lloyd")
k6

```
In the output of our k-means operation, we observe a list with several key information. From this, we conclude the useful information being –

cluster – This is a vector of several integers that denote the cluster which has an allocation of each point.
totss – This represents the total sum of squares.
centers – Matrix comprising of several cluster centers
withinss – This is a vector representing the intra-cluster sum of squares having one component per cluster.
tot.withinss – This denotes the total intra-cluster sum of squares.
betweenss – This is the sum of between-cluster squares.
size – The total number of points that each cluster holds.

## Visualizing the clustering results using the First Two Principle Components

```{r echo=FALSE}

pcclust=prcomp(customer_data[,3:5],scale=FALSE) #principal component analysis
summary(pcclust)

pcclust$rotation[,1:2]

```

## Visualize the cluster
```{r echo=FALSE}

set.seed(1)
ggplot(customer_data, aes(x =Annual.Income..k.., y = Spending.Score..1.100.)) + 
  geom_point(stat = "identity", aes(color = as.factor(k6$cluster))) +
  scale_color_discrete(name=" ",
              breaks=c("1", "2", "3", "4", "5","6"),
              labels=c("Cluster 1", "Cluster 2", "Cluster 3", "Cluster 4", "Cluster 5","Cluster 6")) +
  ggtitle("Segments of Mall Customers", subtitle = "Using K-means Clustering")

```
From the above visualization, we observe that there is a distribution of 6 clusters as follows –

Cluster 6 and 4 – These clusters represent the customer_data with the medium income salary as well as the medium annual spend of salary.

Cluster 1 – This cluster represents the customer_data having a high annual income as well as a high annual spend.

Cluster 3 – This cluster denotes the customer_data with low annual income as well as low yearly spend of income.

Cluster 2 – This cluster denotes a high annual income and low yearly spend.

Cluster 5 – This cluster represents a low annual income but its high yearly expenditure.


```{r echo=FALSE}
ggplot(customer_data, aes(x =Spending.Score..1.100., y =Age)) + 
  geom_point(stat = "identity", aes(color = as.factor(k6$cluster))) +
  scale_color_discrete(name=" ",
                      breaks=c("1", "2", "3", "4", "5","6"),
                      labels=c("Cluster 1", "Cluster 2", "Cluster 3", "Cluster 4", "Cluster 5","Cluster 6")) +
  ggtitle("Segments of Mall Customers", subtitle = "Using K-means Clustering")
```

```{r echo=FALSE}
kCols=function(vec){cols=rainbow (length (unique (vec)))
return (cols[as.numeric(as.factor(vec))])}

digCluster<-k6$cluster; dignm<-as.character(digCluster); # K-means clusters

plot(pcclust$x[,1:2], col =kCols(digCluster),pch =19,xlab ="K-means",ylab="classes")
legend("bottomleft",unique(dignm),fill=unique(kCols(digCluster)))
```

Cluster 5 and 1 – These two clusters consist of customers with medium PCA1 and medium PCA2 score.

Cluster 4 – This cluster represents customers having a high PCA2 and a low PCA1.

Cluster 3 – In this cluster, there are customers with a medium PCA1 and a low PCA2 score.

Cluster 2 – This cluster comprises of customers with a high PCA1 and a high PCA2.

Cluster 6 – This comprises of customers with a high PCA2 and a medium PCA1.

## Summary
With the help of clustering, we can understand the variables much better, prompting us to take careful decisions. With the identification of customers, companies can release products and services that target customers based on several parameters like income, age, spending patterns, etc.


